{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will show you how to organize in 2D a set of documents/articles/posts so that articles with similar content are grouped near to each other. The example I am using is a set of Wikipedia articles of [Political ideologies](https://en.wikipedia.org/wiki/List_of_political_ideologies), but in principle it can be used for any set of documents. \n",
    "\n",
    "The result of this notebook [can be viewed live here](https://www.genekogan.com/works/wiki-tSNE/).\n",
    "\n",
    "### Procedure\n",
    "\n",
    "The pipeline consists of two steps.\n",
    "\n",
    "1) Convert all of the articles to a [tf-idf matrix](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
    "\n",
    "tf-idf stands for \"term frequency inverse document frequency\" and is commonly used in natural language processing applications dealing with large collections of documents. A tf-idf matrix is an $n * m$ sparse matrix consisting of $n$ rows, corresponding to our $n$ documents, and $m$ columns, corresponding to the $m$ unique \"terms\" (usually just words but can be n-grams or other kinds of tokens) that appear in the entire corpus.\n",
    "\n",
    "Each entry in the matrix, $tfidf(i,j)$ can be interpreted as the \"relative importance\" of term $j$ to document $i$.  It is calculated as\n",
    "\n",
    "$$tfidf(i,j) = tf(i,j)*idf(i,j)$$\n",
    "\n",
    "$tf(i, j)$ is the \"term frequency,\" i.e. the percentage of terms in document $i$ which are term $j$. For example, in the document \"the cat in the hat\", the term \"the\" has a $tf$ of (2 / 5) = 0.4. Thus $tf$ is high when the term is frequently found in the document.\n",
    "\n",
    "$idf(i, j)$, not to be confused with [this IDF](https://twitter.com/idfspokesperson/status/547144026445471744) is the \"inverse document frequency.\" It is given by:\n",
    "\n",
    "$$idf(i, j) = log(\\frac{N}{n_j})$$\n",
    "\n",
    "where $N$ is the number of documents in the corpus and $n_j$ is the number of documents which contain term $j$. When $n_j$ is high, this value shrinks towards 0. This happens when the term frequently appears in many or all documents, thus common terms like \"the\", \"a\", \"it\", etc will have a low $idf$ score because they appear in most documents. Conversely, when the term rarely appears in documents ($n_j$ is low), then $idf$ score will be high. These tend to be special or topic-specific words which appear in few of the documents.\n",
    "\n",
    "So intuitively, the $tfidf$ score for a term in a document goes higher if the term appears frequently in the document and appears infrequently in other documents (so that term is important to that document).\n",
    "\n",
    "2) This gives you a high-dimensional matrix of n documents which can be reduced to 2 dimensions using the [t-SNE](https://lvdmaaten.github.io/tsne/) dimensionality reduction technique. A better description of how t-SNE works can be found in the link.\n",
    "\n",
    "### Installation\n",
    "\n",
    "You need [nltk](http://www.nltk.org/install.html) and [scikit-learn](http://scikit-learn.org/) to run most of this notebook.\n",
    "\n",
    "    pip install -U nltk\n",
    "    pip install -U scikit-learn\n",
    "\n",
    "Also, if you don't already have [numpy](http://www.numpy.org/), you should install it as well (it's only used to normalize the data later). \n",
    "\n",
    "    pip install numpy\n",
    "\n",
    "Additionally, if you are following this example and wish to extract articles from Wikipedia, you need the python [wikipedia](https://pypi.python.org/pypi/wikipedia/) library. \n",
    "\n",
    "    pip install wikipedia\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First make sure all these imports work (minus wikipedia if you intend to use another corpus). We will assume wikipedia from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "import time\n",
    "import cPickle as pickle\n",
    "import json\n",
    "import wikipedia\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we are going to cluster all of the links found in the Wikipedia page \"[List of political ideologies](https://en.wikipedia.org/wiki/List_of_political_ideologies).\" First, we will open the page in main, then add all of the raw text of the articles into a dictionary called token_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting text for article 0/410 : Absolute monarchy\n",
      "getting text for article 20/410 : Anti-revisionism\n",
      "getting text for article 40/410 : Bolivarianism\n",
      "getting text for article 60/410 : Christian anarchism\n",
      "getting text for article 80/410 : Confederation\n",
      "getting text for article 100/410 : Cultural liberalism\n",
      "getting text for article 120/410 : Empire\n",
      "getting text for article 140/410 : Fundamentalism\n",
      "getting text for article 160/410 : Ho Chi Minh Thought\n",
      "getting text for article 180/410 : Insurrectionary anarchism\n",
      "getting text for article 200/410 : Kemalism\n",
      "getting text for article 220/410 : Libertarian Marxism\n",
      "getting text for article 240/410 : Masculism\n",
      "getting text for article 260/410 : National Unity government\n",
      "getting text for article 280/410 : One-party state\n",
      " ==> error processing Panarchism\n",
      "getting text for article 300/410 : Party platform\n",
      "getting text for article 320/410 : Progressivism\n",
      "getting text for article 340/410 : Republic\n",
      " ==> error processing Rexism\n",
      "getting text for article 360/410 : Social class\n",
      "getting text for article 380/410 : Theoconservatism\n",
      "getting text for article 400/410 : Utopian socialism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gene/anaconda/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "main = wikipedia.page('List of political ideologies')\n",
    "token_dict = {}\n",
    "for i, article in enumerate(main.links):\n",
    "    if article not in token_dict:\n",
    "        time.sleep(5)  # helps to avoid hangups. Ctrl-C in case you get stuck on one\n",
    "        if i%20==0:\n",
    "            print \"getting text for article %d/%d : %s\"%(i, len(main.links), article)\n",
    "        try:\n",
    "            text = wikipedia.page(article)\n",
    "            token_dict[article] = text.content\n",
    "        except:\n",
    "            print \" ==> error processing \"+article\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find it useful to save the dictionary so as to not have to re-download all the articles later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(token_dict, open(\"token_dict_political_ideologies.p\", \"wb\" ))\n",
    "\n",
    "# later you can retrieve it like this:\n",
    "#token_dict = pickle.load(open(\"token_dict_political_ideologies.p\", \"r\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will evaluate the tf-idf matrix of our collection of articles. First we need to define a tokenizer function which will properly convert all the raw text of the articles into a vector of tokens (our terms).\n",
    "\n",
    "The function `tokenize` will take the raw text, convert it to lower-case, strip out punctuation and special characters, remove all stop words (common words in english, e.g. \"the\", \"and\", \"i\", etc), stem the remaining words ([using Porter stemmer](https://en.wikipedia.org/wiki/Stemming)) to unify tokens in different parts-of-speech (e.g. \"run\", \"running\", \"ran\" --> \"run\"), and output what's left as a vector.\n",
    "\n",
    "From there we run the tfidf vectorizer which will return us the resulting tfidf matrix. Then we use [SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition) to reduce the dimensionality before t-SNE (this is optional, but some sources recommend it). \n",
    "\n",
    "Note, calculating the tfidf matrix can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating tf-idf\n",
      "reducing tf-idf to 500 dim\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    text = text.lower() # lower case\n",
    "    for e in set(string.punctuation+'\\n'+'\\t'): # remove punctuation and line breaks/tabs\n",
    "        text = text.replace(e, ' ')\t\n",
    "    for i in range(0,10):\t# remove double spaces\n",
    "        text = text.replace('  ', ' ')\n",
    "    text = text.translate(string.punctuation)  # punctuation\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    text = [w for w in tokens if not w in stopwords.words('english')] # stopwords\n",
    "    stems = []\n",
    "    for item in tokens: # stem\n",
    "        stems.append(PorterStemmer().stem(item))\n",
    "    return stems\n",
    "\n",
    "# calculate tfidf (might take a while)\n",
    "print \"calculating tf-idf\"\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "tfs = tfidf.fit_transform(token_dict.values())\n",
    "print \"reducing tf-idf to 500 dim\"\n",
    "tfs_reduced = TruncatedSVD(n_components=500, random_state=0).fit_transform(tfs)\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly see what we have computed. Your exact results may vary.  \n",
    "\n",
    "`tfs` is our raw tf-idf matrix.  We can print it and see the values. For example, tfs(0, 20000) is the tfidf score for document 0, term 20000. We can query to find out what this term is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 30109)\t0.00883237481975\n",
      "  (0, 1099)\t0.00981579227116\n",
      "  (0, 18585)\t0.0173205353806\n",
      "  (0, 19820)\t0.0185190161963\n",
      "  (0, 26580)\t0.00989461873953\n",
      "  (0, 1125)\t0.0100588730717\n",
      "  (0, 333)\t0.0137618116339\n",
      "  (0, 6688)\t0.0117129951469\n",
      "  (0, 11325)\t0.0121933714241\n",
      "  (0, 16927)\t0.00618028973016\n",
      "  (0, 6393)\t0.00465224856461\n",
      "  (0, 1838)\t0.00769019698985\n",
      "  (0, 18429)\t0.0140732380778\n",
      "  (0, 20813)\t0.00765254275598\n",
      "  (0, 18045)\t0.00659453758079\n",
      "  (0, 2594)\t0.0185190161963\n",
      "  (0, 10433)\t0.00801005658467\n",
      "  (0, 2)\t0.0145344340567\n",
      "  (0, 4625)\t0.0158535772335\n",
      "  (0, 16236)\t0.00906432258494\n",
      "  (0, 22410)\t0.0174457751699\n",
      "  (0, 11955)\t0.00425924948829\n",
      "  (0, 1728)\t0.00906432258494\n",
      "  (0, 26871)\t0.0164701997093\n",
      "  (0, 24598)\t0.00476275928191\n",
      "  :\t:\n",
      "  (385, 29828)\t0.0587711670585\n",
      "  (385, 7166)\t0.00227550091861\n",
      "  (385, 9263)\t0.00985853068481\n",
      "  (385, 4122)\t0.00326453542147\n",
      "  (385, 15107)\t0.0025366095366\n",
      "  (385, 14047)\t0.00531973808189\n",
      "  (385, 15833)\t0.0102509776583\n",
      "  (385, 28532)\t0.089936916532\n",
      "  (385, 20637)\t0.00357408971114\n",
      "  (385, 19889)\t0.0165399802053\n",
      "  (385, 28825)\t0.0196548878593\n",
      "  (385, 22387)\t0.0126907971631\n",
      "  (385, 28032)\t0.00703645759709\n",
      "  (385, 11622)\t0.0462675356759\n",
      "  (385, 13701)\t0.00257827161128\n",
      "  (385, 30723)\t0.00317067644708\n",
      "  (385, 15556)\t0.005659962089\n",
      "  (385, 10906)\t0.0127843073421\n",
      "  (385, 31032)\t0.0206749752566\n",
      "  (385, 8364)\t0.00220264565257\n",
      "  (385, 18109)\t0.0102785883645\n",
      "  (385, 8374)\t0.00760861644285\n",
      "  (385, 26940)\t0.00300009869602\n",
      "  (385, 28005)\t0.004812232915\n",
      "  (385, 23826)\t0.0654530421537\n",
      "term 20000 = \"melvyn\"\n"
     ]
    }
   ],
   "source": [
    "print tfs\n",
    "print \"term 20000 = \\\"%s\\\"\"%tfidf.get_feature_names()[20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at tfs_reduced, which comes from our truncated SVD. note, we requested 500 dims, but it returned 408 because that's how many documents we managed to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of TSVD: (386, 386)\n",
      "[[  6.29518411e-01   1.59304474e-01  -3.79648006e-01 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  4.02527409e-01   1.37833192e-01  -2.74399646e-01 ...,   4.69879382e-33\n",
      "   -1.86184512e-32  -4.80886726e-33]\n",
      " [  1.65900711e-01  -6.24006739e-02   4.08613097e-02 ...,  -4.31602180e-32\n",
      "    1.56216425e-32   9.80053863e-33]\n",
      " ..., \n",
      " [  4.82534999e-01  -3.44946433e-02   1.23875202e-01 ...,   1.55345624e-17\n",
      "   -2.03104533e-18   8.05032871e-19]\n",
      " [  1.01249033e-01   8.90437471e-03   1.95603819e-02 ...,   1.29216830e-32\n",
      "    1.68020169e-32  -1.77168794e-33]\n",
      " [  2.54122504e-01  -3.39835124e-02   3.63681354e-02 ...,  -3.99397475e-32\n",
      "    3.17875996e-33   1.06301276e-32]]\n"
     ]
    }
   ],
   "source": [
    "print \"size of TSVD: \"+str(tfs_reduced.shape)\n",
    "print tfs_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run t-SNE on the result, normalize the t-SNE results (for convenience), and save the final coordinates to a json file.\n",
    "\n",
    "Notice that we run it on `tfs_reduced`. You can also run it on the original tf-idf matrix (results should be similar). Make sure to remember to make it a dense matrix, i.e. `tfs.todense()`.\n",
    "\n",
    "You may also want to play a bit with the parameters, like the size of the SVD reduction (probably minor effect) or the perplexity of the t-SNE.\n",
    "\n",
    "At this point, the 2d normalized t-SNE points have been saved to a JSON file, along with the corresponding names of the articles. A nice way to visualize it is to display the terms in the browser. I've made a p5.js sketch which does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing pairwise distances...\n",
      "[t-SNE] Computed conditional probabilities for sample 386 / 386\n",
      "[t-SNE] Mean sigma: 0.310351\n",
      "[t-SNE] Iteration 10: error = 21.1211239, gradient norm = 0.0656523\n",
      "[t-SNE] Iteration 20: error = 19.1175275, gradient norm = 0.0897504\n",
      "[t-SNE] Iteration 30: error = 18.6889811, gradient norm = 0.0844317\n",
      "[t-SNE] Iteration 32: did not make any progress during the last 30 episodes. Finished.\n",
      "[t-SNE] Iteration 40: error = 19.3492974, gradient norm = 0.0699504\n",
      "[t-SNE] Iteration 50: error = 18.6104129, gradient norm = 0.0703584\n",
      "[t-SNE] Iteration 60: error = 18.9324655, gradient norm = 0.0649455\n",
      "[t-SNE] Iteration 66: did not make any progress during the last 30 episodes. Finished.\n",
      "[t-SNE] Error after 66 iterations with early exaggeration: 18.050238\n",
      "[t-SNE] Iteration 70: error = 2.5665829, gradient norm = 0.0151954\n",
      "[t-SNE] Iteration 80: error = 1.8550274, gradient norm = 0.0193772\n",
      "[t-SNE] Iteration 90: error = 1.5864296, gradient norm = 0.0194727\n",
      "[t-SNE] Iteration 100: error = 1.4678455, gradient norm = 0.0197867\n",
      "[t-SNE] Iteration 110: error = 1.4074930, gradient norm = 0.0181079\n",
      "[t-SNE] Iteration 120: error = 1.4257048, gradient norm = 0.0181708\n",
      "[t-SNE] Iteration 130: error = 1.4239754, gradient norm = 0.0184936\n",
      "[t-SNE] Iteration 140: error = 1.5135613, gradient norm = 0.0188684\n",
      "[t-SNE] Iteration 141: did not make any progress during the last 30 episodes. Finished.\n",
      "[t-SNE] Error after 141 iterations: 1.508585\n"
     ]
    }
   ],
   "source": [
    "#model = TSNE(n_components=2, perplexity=50, verbose=2).fit_transform(tfs.todense())\n",
    "model = TSNE(n_components=2, perplexity=50, verbose=2).fit_transform(tfs_reduced)\n",
    "\n",
    "# save to json file\n",
    "x_axis=model[:,0]\n",
    "y_axis=model[:,1]\n",
    "x_norm = (x_axis-np.min(x_axis)) / (np.max(x_axis) - np.min(x_axis))\n",
    "y_norm = (y_axis-np.min(y_axis)) / (np.max(y_axis) - np.min(y_axis))\n",
    "data = {\"x\":x_norm.tolist(), \"y\":y_norm.tolist(), \"names\":token_dict.keys()}\n",
    "with open('data_political_ideologies.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We now have put the t-SNE normalized coordinates and document names into the file \"data_political_ideologies.json\" so they can be used in other environments. This codebase comes with an example (in the `visualize` folder) of displaying these in a webpage using [p5.js](http://www.p5js.org) (javascript). This script uses a simple collision detection procedure to space out all the text so they are not overlapping. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
